import random
from dataclasses import dataclass
from .config import DatasetConfig

import torch
import sentencepiece as spm  # type: ignore


class Encoder:
    """
    An encoder defines mappings between strings and lists of token indices.
    """

    def encode(self, input: str) -> list[int]:
        """
        Encodes the input string into a list of token indices.
        """
        raise NotImplementedError()

    def decode(self, input: list[int]) -> str:
        """
        Decodes the input list of token indices into a string.
        """
        raise NotImplementedError()

    def vocab_size(self) -> int:
        """
        The size of the vocabulary. All token indices are to be between 0 (inclusive)
        and ``vocab_size`` (exclusive).
        """
        raise NotImplementedError()


class CharEncoder(Encoder):
    """
    Encoder which directly encodes characters into indices.
    """

    _char_to_index: dict[str, int]
    _index_to_char: list[str]

    def __init__(self, chars: str) -> None:
        """
        Initializes the encoder.

        Args:
            chars: String containing all possible input characters that might need to
                be encoded by this encoder.
        """
        super().__init__()
        self._index_to_char = sorted(set(chars))
        self._char_to_index = {ch: i for i, ch in enumerate(self._index_to_char)}

    def encode(self, input: str) -> list[int]:
        return [self._char_to_index[ch] for ch in input]

    def decode(self, input: list[int]) -> str:
        return "".join([self._index_to_char[i] for i in input])

    def vocab_size(self) -> int:
        return len(self._index_to_char)


class SentencePieceEncoder(Encoder):
    """
    Encoder which uses SentencePiece to encode strings into subword units.
    """

    _processor: spm.SentencePieceProcessor
    _eos_id: int

    def __init__(self, processor: spm.SentencePieceProcessor) -> None:
        """
        Initializes the encoder.

        Args:
            processor: The SentencePiece processor that has been initialized with a
                SentencePiece model.
        """
        super().__init__()
        self._processor = processor
        self._eos_id = processor.eos_id()

    def encode(self, input: str) -> list[int]:
        lines = input.split("\n")
        result = []
        for i, encoded_line in enumerate(self._processor.encode(lines)):
            if i > 0:
                result.append(self._eos_id)
            result.extend(encoded_line)
        return result

    def decode(self, input: list[int]) -> str:
        return "\n".join(self._processor.decode(_split_list(input, self._eos_id)))

    def vocab_size(self) -> int:
        return self._processor.vocab_size()


def _split_list(input: list[int], delimiter: int) -> list[list[int]]:
    offsets = [i for i, val in enumerate(input) if val == delimiter]
    last_offset = 0
    result = []
    for offset in offsets:
        result.append(input[last_offset:offset])
        last_offset = offset
    result.append(input[last_offset:])
    return result


class Dataset:
    """
    A dataset that can be used to train or evaluate a transformer model.
    """

    _chunks: torch.Tensor

    def __init__(self, chunks: torch.Tensor) -> None:
        """
        Initializes the dataset.

        Args:
            chunks: The list of equal-length chunks comprising the dataset. The chunks
                are encoded as a 2d tensor with shape ``(m, n)`` where ``m`` is the
                number of chunks and ``n`` is the length of each chunk. The elements of
                the tensor should be token indices generated by an Encoder.
        """
        self._chunks = chunks

    def get_batch(
        self, n_batch: int, n_context: int
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Gets a randomly selected batch from the dataset.

        Args:
            n_batch: The size of the batch.
            n_context: The context length of each element within the batch.

        Returns:
            A tuple of ``(xs, ys)`` where ``xs`` is a tensor containing inputs for the
            batch and ``ys`` is a tensor containing outputs for the batch, selected
            randomly from the chunks within the dataset. Both tensors should have shape
            ``(n_batch, n_context)``.
        """
        # Compute chunk indexes.
        ixs = torch.randint(self._chunks.size(0), (n_batch,), device="cpu")

        # Compute offsets into each chunk.
        x_starts = torch.randint(
            self._chunks.size(1) - n_context - 1,
            (n_batch,),
            device="cpu",
        )

        # Slice chunks to produce inputs and outputs.
        xs = []
        ys = []
        for ix, x_start in zip(ixs.tolist(), x_starts.tolist()):
            x_stop = x_start + n_context
            y_start = x_start + 1
            y_stop = x_stop + 1
            xs.append(self._chunks[ix, x_start:x_stop])
            ys.append(self._chunks[ix, y_start:y_stop])

        # Stack slices to generate input and output tensors.
        return torch.stack(xs), torch.stack(ys)

    @property
    def chunk_count(self):
        return len(self._chunks)


@dataclass
class Datasets:
    """
    Training, validation, and test dataset splits.
    """

    config: DatasetConfig
    encoder: Encoder
    train: Dataset
    val: Dataset
    test: Dataset


def create_datasets(config: DatasetConfig) -> Datasets:
    """
    Creates dataset splits for a piece of text.

    The input text is first split into equal-sized chunks of a specified size. Those
    chunks are then shuffled randomly into the dataset splits.

    Args:
        config: The config specifying how the dataset splits should be generated.

    Returns:
        Dataset splits generated from the text.
    """

    # Seed the RNG deterministically.
    rng = random.Random(config.random_seed)

    # Read the input text.
    with open(config.input_file, "rt", encoding=config.input_file_encoding) as f:
        text = f.read()

    # Create the encoder.
    if config.sentencepiece_model_file is not None:
        encoder: Encoder = SentencePieceEncoder(
            spm.SentencePieceProcessor(model_file=config.sentencepiece_model_file)
        )
    else:
        encoder = CharEncoder(text)

    # Encode the text.
    tokens = encoder.encode(text)

    # Split encoded text into equal-sized chunks.
    chunks = [
        torch.tensor(tokens[i : i + config.tokens_per_chunk], dtype=torch.long)
        for i in range(0, len(tokens), config.tokens_per_chunk)
        # N.B., all chunks must be exactly the same size, so we throw out any odd-sized
        # pieces.
        if i + config.tokens_per_chunk <= len(tokens)
    ]

    # Shuffle chunks.
    rng.shuffle(chunks)

    # Split into train/val/test.
    chunks_train = chunks[: int(len(chunks) * config.fraction_train)]
    chunks_val = chunks[
        len(chunks_train) : len(chunks_train) + int(len(chunks) * config.fraction_val)
    ]
    chunks_test = chunks[len(chunks_train) + len(chunks_val) :]

    if len(chunks_train) == 0 or len(chunks_val) == 0 or len(chunks_test) == 0:
        raise ValueError(
            "input text length too small to generate sufficient chunks for "
            "dataset splits"
        )

    return Datasets(
        config=config,
        encoder=encoder,
        train=Dataset(torch.stack(chunks_train)),
        val=Dataset(torch.stack(chunks_val)),
        test=Dataset(torch.stack(chunks_test)),
    )
