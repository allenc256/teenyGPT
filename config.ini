# Dataset config (maps to teenygpt.DatasetConfig)
[dataset]

# Input file and encoding.
input_file=data/tiny_shakespeare.txt
input_file_encoding=utf-8-sig

# Sentence piece model to use to convert text into tokens.
# If unspecified, then a character-level model will be used.
#sentencepiece_model_file=models/sentencepiece/tiny_shakespeare_512.model

# Number of tokens in each chunk that input text is split into.
tokens_per_chunk=10000

# Fraction of chunks that should be assigned to training dataset split.
fraction_train=0.8

# Fraction of chunks that should be assigned to validation dataset split.
fraction_val=0.1

# Random seed used to guarantee determinism in dataset generation.
random_seed=0

# Model config (maps to teenygpt.ModelConfig)
[model]

# Model embedding dimension size.
n_dim=256

# Number of attention heads.
n_attn_heads=8

# Number of attention layers.
n_attn_layers=4

# Dropout probability.
p_dropout=0.2

# Positional encoding type to use; one of:
# * NONE
# * ALIBI
# * SINUSOIDAL
# * LEARNED_EMBEDDING
# * LEARNED_SINUSOIDAL
# * ROPE
positional_encoding=LEARNED_SINUSOIDAL

# The maximum context length that the model can handle.
n_context_max=256

# Training config (maps to teenygpt.TrainConfig)
[train]

# Number of training iterations.
n_iters=2000

# Number of tokens in context window.
n_context=256

# Batch size.
n_batch=64

# Number of iterations to execute before estimating loss.
n_est_loss_iters=100

# Number of batches to use to estimate loss.
n_est_loss_batches=4

# File that training checkpoints should be written to.
checkpoint_file=models/teenygpt/tiny_shakespeare.pt
